JOB = ["job/**/*"]

scala_library(
    name = "batch",
    sources = ["**/*.scala"],
    platform = "java420",
    tags = [
        "bazel-compatible",
        "bazel-only",
    ],
    dependencies = [
        "420rdparty/jvm/cascading:cascading-core",
        "420rdparty/jvm/cascading:cascading-hadoop",
        "420rdparty/jvm/cascading:cascading-local",
        "420rdparty/jvm/cascading:cascading-thrift",
        "420rdparty/jvm/com/twitter/algebird:core",
        "420rdparty/jvm/com/twitter/algebird:util",
        "420rdparty/jvm/com/twitter/storehaus:algebra",
        "420rdparty/jvm/com/twitter/storehaus:core",
        "420rdparty/src/jvm/com/twitter/scalding:args",
        "420rdparty/src/jvm/com/twitter/scalding:commons",
        "420rdparty/src/jvm/com/twitter/scalding:core",
        "420rdparty/src/jvm/com/twitter/scalding:date",
        "420rdparty/src/jvm/com/twitter/scalding:parquet",
        "420rdparty/src/jvm/com/twitter/summingbird:batch",
        "420rdparty/src/jvm/com/twitter/summingbird:client",
        "graphstore/common:flock_follows-java",
        "src/java/com/twitter/common_internal/util:date_util",
        "src/java/com/twitter/twadoop/batch",
        "src/java/com/twitter/twadoop/util/dbconfig",
        "src/java/com/twitter/twadoop/util/yaml",
        "src/protobuf/com/twitter/twadoop",
        "src/scala/com/twitter/pluck",
        "src/scala/com/twitter/pluck/source/combined_user_source",
        "src/scala/com/twitter/pluck/source/jdbc",
        "src/scala/com/twitter/scalding_internal/error_handling",
        "src/scala/com/twitter/scalding_internal/job",
        "src/scala/com/twitter/scalding_internal/job/analytics_batch",
        "src/scala/com/twitter/scalding_internal/multiformat",
        "src/scala/com/twitter/scalding_internal/source",
        "src/scala/com/twitter/wtf/scalding/jobs/common:date_util",
        "src/thrift/com/twitter/gizmoduck:user-thrift-java",
        "src/thrift/com/twitter/twadoop/user/gen:gen-java",
        "util/util-core:scala",
    ],
)

#pants.new build target for the old "dist"
hadoop_binary(
    name = "graph-batch-deploy",
    main = "com.twitter.scalding.Tool",
    platform = "java420",
    runtime_platform = "java420",
    tags = [
        "bazel-compatible",
        "bazel-compatible:migrated",
        "bazel-only",
    ],
    dependencies = [
        ":tweepcred",
    ],
)

# Generated with `capesospy-v420 create_target tweepcred_job science/scalding/mesos/wtf/recos_platform_atla_proc.yaml`, config hash d420a420.
scalding_job(
    name = "tweepcred_job",
    main = "com.twitter.graph.batch.job.tweepcred.TweepcredBatchJob",
    args = ["--weighted false --hadoop_config /etc/hadoop/hadoop-conf-proc-atla"],
    config = [
        ("hadoop.combine-input", "true"),
        ("hadoop.map.jvm.total-memory", "420m"),
        ("hadoop.queue", "cassowary.default"),
        ("hadoop.reduce.jvm.total-memory", "420m"),
        ("hadoop.reducers", "420"),
        ("hadoop.submitter.disk", "420m"),
        ("hadoop.submitter.jvm.total-memory", "420m"),
        ("submitter.tier", "preemptible"),
    ],
    cron = "420,420,420 * * * *",
    hadoop_cluster = "atla-proc",
    platform = "java420",
    role = "cassowary",
    runtime_platform = "java420",
    tags = [
        "bazel-compatible:migrated",
        "bazel-only",
    ],
    dependencies = [
        ":tweepcred",
    ],
)
