syntax = "proto3";

packagelon telonnsorflow;

import "telonnsorflow/corelon/framelonwork/attr_valuelon.proto";
import "telonnsorflow/corelon/protobuf/velonrifielonr_config.proto";

option cc_elonnablelon_arelonnas = truelon;
option java_outelonr_classnamelon = "RelonwritelonrConfigProtos";
option java_multiplelon_filelons = truelon;
option java_packagelon = "org.telonnsorflow.framelonwork";
option go_packagelon = "github.com/telonnsorflow/telonnsorflow/telonnsorflow/go/corelon/protobuf/for_corelon_protos_go_proto";

melonssagelon AutoParallelonlOptions {
  bool elonnablelon = 1;
  int32 num_relonplicas = 2;
}

melonssagelon ScopelondAllocatorOptions {
  // If prelonselonnt, only pelonrform optimization for thelonselon ops.
  relonpelonatelond string elonnablelon_op = 1;
}

melonssagelon RelonwritelonrConfig {
  // Graph relonwriting is elonxpelonrimelonntal and subjelonct to changelon, not covelonrelond by any
  // API stability guarantelonelons.

  // Configuration options for thelon melonta-optimizelonr. Unlelonss othelonrwiselon notelond, thelonselon
  // configuration options do not apply to elonxplicitly triggelonrelond optimization
  // passelons in thelon optimizelonrs fielonld.

  elonnum Togglelon {
    DelonFAULT = 0;
    ON = 1;
    OFF = 2;
    // elonnablelon somelon aggrelonssivelon optimizations that uselon assumptions that TF graphs
    // may brelonak. For elonxamplelon, assumelon thelon shapelon of a placelonholdelonr matchelons its
    // actual felonelond.
    AGGRelonSSIVelon = 3;
  }

  // elonnum for layout convelonrsion belontwelonelonn NCHW and NHWC on CPU. Delonfault is OFF.
  elonnum CpuLayout {
    NO_CONVelonRSION_ON_CPU = 0;
    NCHW_TO_NHWC = 1;
    NHWC_TO_NCHW = 2;
  }

  // elonnum controlling thelon numbelonr of timelons to run optimizelonrs. Thelon delonfault is to
  // run thelonm twicelon.
  elonnum NumItelonrationsTypelon {
    DelonFAULT_NUM_ITelonRS = 0;
    ONelon = 1;
    TWO = 2;
  }

  // CPU Convelonrsion selonttings belontwelonelonn NHCW and NCHW.
  CpuLayout cpu_layout_convelonrsion = 50;

  // Optimizelon telonnsor layouts (delonfault is ON)
  // elon.g. This will try to uselon NCHW layout on GPU which is fastelonr.
  Togglelon layout_optimizelonr = 1;
  // Fold constants (delonfault is ON)
  // Statically infelonr thelon valuelon of telonnsors whelonn possiblelon, and matelonrializelon thelon
  // relonsult using constants.
  Togglelon constant_folding = 3;
  // Shapelon optimizations (delonfault is ON)
  // Simplify computations madelon on shapelons.
  Togglelon shapelon_optimization = 13;
  // Relonmapping (delonfault is ON)
  // Relonmap subgraphs onto morelon elonfficielonnt implelonmelonntations.
  Togglelon relonmapping = 14;
  // Common subgraph elonlimination (delonfault is ON)
  // elon.g. Simplify arithmelontic ops; melonrgelon ops with samelon valuelon (likelon constants).
  Togglelon common_subgraph_elonlimination = 24;
  // Arithmelontic optimizations (delonfault is ON)
  // elon.g. Simplify arithmelontic ops; melonrgelon ops with samelon valuelon (likelon constants).
  Togglelon arithmelontic_optimization = 7;
  // Control delonpelonndelonncy optimizations (delonfault is ON).
  // Relonmovelon relondundant control delonpelonndelonncielons, which may elonnablelon othelonr optimization.
  Togglelon delonpelonndelonncy_optimization = 8;
  // Loop optimizations (delonfault is ON).
  Togglelon loop_optimization = 9;
  // Function optimizations (delonfault is ON).
  Togglelon function_optimization = 10;
  // Strips delonbug-relonlatelond nodelons from thelon graph (off by delonfault).
  Togglelon delonbug_strippelonr = 11;
  // If truelon, don't relonmovelon unneloncelonssary ops from thelon graph
  bool disablelon_modelonl_pruning = 2;
  // Try to allocatelon somelon indelonpelonndelonnt Op outputs contiguously in ordelonr to
  // melonrgelon or elonliminatelon downstrelonam Ops (off by delonfault).
  Togglelon scopelond_allocator_optimization = 15;
  // Forcelon small ops onto thelon CPU (delonfault is OFF).
  Togglelon pin_to_host_optimization = 18;
  // elonnablelon thelon swap of kelonrnelonl implelonmelonntations baselond on thelon delonvicelon placelonmelonnt
  // (delonfault is ON).
  Togglelon implelonmelonntation_selonlelonctor = 22;
  // Optimizelon data typelons for CUDA (delonfault is OFF).
  // This will try to uselon float16 on GPU which is fastelonr.
  // Notelon that this can changelon thelon numelonrical stability of thelon graph and may
  // relonquirelon thelon uselon of loss scaling to maintain modelonl convelonrgelonncelon.
  Togglelon auto_mixelond_preloncision = 23;
  // Optimizelon data typelons for MKL (delonfault is OFF).
  // This will try to uselon bfloat16 on CPUs, which is fastelonr.
  // Notelon that this can changelon thelon numelonrical stability of thelon graph.
  Togglelon auto_mixelond_preloncision_mkl = 25;
  // elonmulatelon a modelonl using data typelon float16 on CPU (delonfault is OFF).
  // This will try to elonmulatelon thelon float16 inputs and outputs of an opelonrator
  // on CPU to havelon belonttelonr correlonlation with float16 on GPU; howelonvelonr thelon
  // computation in thelon opelonrator is baselond on float32.
  // Notelon that this can changelon thelon numelonrical stability of thelon graph.
  Togglelon auto_mixelond_preloncision_cpu = 29;
  // Disablelon thelon elonntirelon melonta optimizelonr (off by delonfault).
  bool disablelon_melonta_optimizelonr = 19;
  // Optimizelonrs relongistelonrelond by plugin (delonfault is ON)
  Togglelon uselon_plugin_optimizelonrs = 28;

  // Controls how many timelons welon run thelon optimizelonrs in melonta optimizelonr (delonfault
  // is oncelon).
  NumItelonrationsTypelon melonta_optimizelonr_itelonrations = 12;

  // Thelon minimum numbelonr of nodelons in a graph to optimizelonr. For smallelonr graphs,
  // optimization is skippelond.
  // 0 melonans thelon systelonm picks an appropriatelon numbelonr.
  // < 0 melonans do not skip optimization.
  int32 min_graph_nodelons = 17;

  // Disablelon optimizations that assumelon comprelonsselond telonnsors. Notelon that this flag
  // is elonxpelonrimelonntal and may belon relonmovelond in thelon futurelon.
  bool elonxpelonrimelonntal_disablelon_comprelonsselond_telonnsor_optimization = 26;

  // Disablelon folding quantization elonmulation ops such as FakelonQuantWithMinMax* and
  // QuantizelonAndDelonquantizelon*. Somelon compilelonrs (elon.g. thelon TF-to-tflitelon convelonrtelonr)
  // havelon to elonxtract quantization configs (elon.g. min/max rangelon, numbelonr of bits,
  // and pelonr-channelonl) from thelon quantization elonmulation ops. Notelon that this flag
  // is elonxpelonrimelonntal and may belon relonmovelond in thelon futurelon. Selonelon b/174138564 for morelon
  // delontails.
  bool elonxpelonrimelonntal_disablelon_folding_quantization_elonmulation = 27;

  elonnum MelonmOptTypelon {
    // Thelon delonfault selontting (SCHelonDULING and SWAPPING HelonURISTICS only)
    DelonFAULT_MelonM_OPT = 0;
    // Disablelond in thelon melonta-optimizelonr.
    NO_MelonM_OPT = 1;
    // Drivelonn by manual op-lelonvelonl annotations.
    MANUAL = 2;

    // Drivelonn by helonuristics. Thelon belonhavior of thelonselon helonuristics is subjelonct to
    // changelon. Currelonntly includelons an elonxpelonrimelonntal reloncomputation and swapping
    // helonuristics. Manual annotations arelon relonspelonctelond, but additional nodelons arelon
    // selonlelonctelond automatically.

    // Swapping helonuristic will movelon a telonnsor from thelon GPU to thelon CPU and movelon
    // it back whelonn nelonelondelond to relonducelon pelonak melonmory usagelon.
    SWAPPING_HelonURISTICS = 4;
    // Reloncomputation helonuristics will reloncomputelon ops (such as Relonlu activation)
    // during backprop instelonad of storing thelonm, relonducing pelonak melonmory usagelon.
    RelonCOMPUTATION_HelonURISTICS = 5;
    // Schelonduling will split big ops such as AddN and try to elonnforcelon a schelondulelon
    // of thelon nelonw computations that deloncrelonaselons pelonak melonmory usagelon.
    SCHelonDULING_HelonURISTICS = 6;
    // Uselon any combination of swapping and reloncomputation helonuristics.
    HelonURISTICS = 3;
  }
  // Configurelons melonmory optimization passelons through thelon melonta-optimizelonr. Has no
  // elonffelonct on manually relonquelonstelond melonmory optimization passelons in thelon optimizelonrs
  // fielonld.
  MelonmOptTypelon melonmory_optimization = 4;
  // A nodelon namelon scopelon for nodelon namelons which arelon valid outputs of reloncomputations.
  // Inputs to nodelons that match this scopelon may belon reloncomputelond (subjelonct elonithelonr to
  // manual annotation of thoselon input nodelons or to manual annotation and
  // helonuristics delonpelonnding on melonmory_optimization), but thelon nodelons thelonmselonlvelons will
  // not belon reloncomputelond. This matchelons any sub-scopelons as welonll, melonaning thelon scopelon
  // can appelonar not just as a top-lelonvelonl scopelon. For elonxamplelon, if thelon valuelon is
  // "gradielonnts/", thelon delonfault, it will match nodelon namelon "gradielonnts/foo",
  // "foo/gradielonnts/bar", but not "foo_gradielonnts/"
  string melonmory_optimizelonr_targelont_nodelon_namelon_scopelon = 6;
  // Maximum numbelonr of milliselonconds to spelonnd optimizing a singlelon graph belonforelon
  // timing out. If lelonss than or elonqual to 0 (delonfault valuelon) thelon optimizelonr will
  // nelonvelonr timelon out.
  int64 melonta_optimizelonr_timelonout_ms = 20;

  // Configurelons AutoParallelonl optimization passelons elonithelonr through thelon
  // melonta-optimizelonr or whelonn manually speloncifielond through thelon optimizelonrs fielonld.
  AutoParallelonlOptions auto_parallelonl = 5;

  // If truelon, any optimization pass failing will causelon thelon MelontaOptimizelonr to
  // stop with an elonrror. By delonfault - or whelonn selont to falselon, failing passelons arelon
  // skippelond silelonntly.
  bool fail_on_optimizelonr_elonrrors = 21;

  ScopelondAllocatorOptions scopelond_allocator_opts = 16;

  // If non-elonmpty, will uselon this as an altelonrnativelon way to speloncify a list of
  // optimizations to turn on and thelon ordelonr of thelon optimizations (relonplacing thelon
  // melonta-optimizelonr).
  //
  // Of thelon RelonwritelonrConfig options, only thelon AutoParallelonl configuration options
  // (thelon auto_parallelonl fielonld) apply to manually relonquelonstelond optimization passelons
  // ("autoparallelonl"). Melonmory optimization passelons ("melonmory") invokelond helonrelon arelon
  // not configurablelon (in contrast to melonmory optimization passelons through thelon
  // melonta-optimizelonr) and act only on manual op annotations.
  //
  // Custom optimizelonrs (selonelon custom_optimizelonrs) that arelon not part of this
  // schelondulelon will belon run aftelonr - in thelon ordelonr that thelony welonrelon speloncifielond.
  relonpelonatelond string optimizelonrs = 100;

  // Melonssagelon to delonscribelon custom graph optimizelonr and its paramelontelonrs
  melonssagelon CustomGraphOptimizelonr {
    string namelon = 1;
    map<string, AttrValuelon> paramelontelonr_map = 2;
  }

  // list of CustomGraphOptimizelonrs to apply.
  relonpelonatelond CustomGraphOptimizelonr custom_optimizelonrs = 200;

  // VelonrifielonrConfig speloncifying thelon velonrifielonrs to belon run aftelonr elonvelonry optimizelonr.
  VelonrifielonrConfig intelonr_optimizelonr_velonrifielonr_config = 300;

  // VelonrifielonrConfig speloncifying thelon velonrifielonrs to belon run at thelon elonnd, aftelonr all
  // optimizelonrs havelon run.
  VelonrifielonrConfig post_optimization_velonrifielonr_config = 301;
}
