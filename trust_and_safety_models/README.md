Trust and Safety Models
=======================

We are excited to announce that we have decided to open source the training code for the following Trust and Safety models, as part of our commitment to transparency and collaboration:

1. pNSFWMedia: This model is designed to detect tweets that contain NSFW (Not Safe for Work) images, including adult and pornographic content. By open sourcing this model, we aim to enable developers and researchers to contribute to the ongoing efforts in detecting and mitigating inappropriate visual content on social media platforms.

2. pNSFWText: Our pNSFWText model is specifically built to identify tweets that contain NSFW text, including discussions about adult or sexual topics. Opening up the training code for this model allows the community to help enhance the detection and filtering mechanisms for sensitive textual content.

3. pToxicity: The pToxicity model focuses on detecting toxic tweets, which encompass various forms of harmful behavior, such as insults and certain types of harassment. It's important to note that toxic content identified by this model does not necessarily violate Twitter's terms of service, but it helps in flagging content that may require further human review or user-specific filtering.

4. pAbuse: The pAbuse model plays a crucial role in identifying abusive content on Twitter. This includes instances that violate Twitter's terms of service, such as hate speech, targeted harassment, and abusive behavior. By sharing the training code for this model, we aim to empower the community to collaborate in building safer online spaces and combatting online abuse.

While we are open sourcing these models, it's worth mentioning that there are additional models and rules that we are not making open source at this time. This decision is primarily driven by the adversarial nature of this domain, where maintaining the effectiveness and integrity of certain trust and safety mechanisms requires more controlled access. However, our team is actively considering open sourcing more models in the future, and we will keep the community informed about any updates or developments in this regard.

By making these models open source, we strive to foster collaboration, advance research, and promote the collective efforts towards creating safer and more inclusive online platforms. We encourage the community to contribute, provide feedback, and join us in shaping the future of trust and safety in social media environments.

