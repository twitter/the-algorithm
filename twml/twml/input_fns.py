'\nContains implementations of functions to read input data.\n'
_B=False
_A=None
from .dataset import stream_block_format_dataset
import tensorflow.compat.v1 as tf
def data_record_input_fn(files,batch_size,parse_fn,num_threads=2,repeat=_B,dataset_fn=_A,keep_rate=_A,parts_downsampling_rate=_A,shards=_A,shard_index=_A,shuffle=True,shuffle_files=True,interleave=True,initializable=_B,log_tf_data_summaries=_B,**H):
	"\n  Returns a nested structure of tf.Tensors containing the next element.\n  Used by ``train_input_fn`` and ``eval_input_fn`` in DataRecordTrainer.\n  By default, works with DataRecord dataset for compressed partition files.\n\n  Args:\n    files:\n      List of files that will be parsed.\n    batch_size:\n      number of samples per batch.\n    parse_fn:\n      function passed to data loading for parsing individual data records.\n      Usually one of the decoder functions like ``parsers.get_sparse_parse_fn``.\n    num_threads (optional):\n      number of threads used for loading data. Defaults to 2.\n    repeat (optional):\n      Repeat the dataset indefinitely. Defaults to False.\n      Useful when you want to use ``train_steps`` or ``eval_steps``\n      greater than the size of the dataset\n      (otherwise Estimator.[train,evaluate] stops when the end of the dataset is reached).\n    dataset_fn (optional):\n      A function that modifies the dataset after it reads different interleaved parts files.\n      Defaults to:\n\n      .. code-block:: python\n\n        def dataset_fn(dataset, parse_fn, batch_size):\n          return dataset.batch(batch_size).map(parse_fn, 1)\n\n    keep_rate (optional):\n      A float value in (0.0, 1.0] that indicates to drop records according to the Bernoulli\n      distribution with p = 1 - keep_rate.\n      Defaults to None (no records dropped).\n\n    parts_downsampling_rate (optional):\n      A float value in (0.0, 1.0] that indicates the factor by which to downsample part files.\n      For example, a value of 0.2 means only 20 percent of part files become part of the dataset.\n\n    shards (optional):\n      Number of partitions to shard the dataset into. This is useful for codistillation\n      (https://arxiv.org/pdf/1804.03235.pdf) and other techniques that require each worker to\n      train on disjoint partitions of the dataset.\n      The dataset is not sharded by default.\n\n    shard_index (optional):\n      Which partition of the dataset to use if ``shards`` is set.\n\n    shuffle (optional):\n      Whether to shuffle the records. Defaults to True.\n\n    shuffle_files (optional):\n      Shuffle the list of files. Defaults to True.\n      When False, files are iterated in the order they are passed in.\n\n    interleave (optional):\n      Interleave records from multiple files in parallel. Defaults to True.\n\n    initializable (optional):\n      A boolean indicator. When the Dataset Iterator depends on some resource, e.g. a HashTable or\n      a Tensor, i.e. it's an initializable iterator, set it to True. Otherwise, default value (false)\n      is used for most plain iterators.\n\n      log_tf_data_summaries (optional):\n        A boolean indicator denoting whether to add a `tf.data.experimental.StatsAggregator` to the\n        tf.data pipeline. This adds summaries of pipeline utilization and buffer sizes to the output\n        events files. This requires that `initializable` is `True` above.\n\n  Returns:\n    Iterator of elements of the dataset.\n  ";C=log_tf_data_summaries;D=initializable;E=parse_fn
	if not E:raise ValueError('default_input_fn requires a parse_fn')
	if C and not D:raise ValueError('Require `initializable` if `log_tf_data_summaries`.')
	A=stream_block_format_dataset(files=files,parse_fn=E,batch_size=batch_size,repeat=repeat,num_threads=num_threads,dataset_fn=dataset_fn,keep_rate=keep_rate,parts_downsampling_rate=parts_downsampling_rate,shards=shards,shard_index=shard_index,shuffle=shuffle,shuffle_files=shuffle_files,interleave=interleave,**H)
	if C:F=tf.data.experimental.StatsAggregator();G=tf.data.Options();G.experimental_stats.aggregator=F;A=A.with_options(G);I=F.get_summary();tf.add_to_collection(tf.GraphKeys.SUMMARIES,I)
	if D:B=A.make_initializable_iterator();tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS,B.initializer)
	else:B=A.make_one_shot_iterator()
	return B.get_next()
default_input_fn=data_record_input_fn