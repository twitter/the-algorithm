'\nImplementing Full Sparse Layer\n'
_C=False
_B=True
_A=None
import math
from twitter.deepbird.sparse import sparse_dense_matmul
from .layer import Layer
import tensorflow.compat.v1 as tf,twml
class FullSparse(Layer):
	"Fully-sparse layer class.\n  This layer implements the operation:\n\n  .. code-block:: python\n\n    outputs = activation(inputs.weight + bias)\n\n  Arguments:\n    output_size:\n      Long or Integer, dimensionality of the output space.\n    input_size:\n      The number of input units. (Deprecated)\n    weight_initializer:\n      Initializer function for the weight matrix.\n      This argument defaults to zeros_initializer().\n      This is valid when the FullSparse is the first layer of\n      parameters but should be changed otherwise.\n    weight_regularizer:\n      Regularizer function for the weight matrix.\n      Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.\n    bias_regularizer:\n      Regularizer function for the bias.\n      Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect\n    activation:\n      Activation function (callable). Set it to None to maintain a linear activation.\n    bias_initializer:\n      Initializer function for the bias.\n      This argument defaults to tf.constant_initializer(1/output_size)\n    trainable:\n      Boolean, if `True` also add variables to the graph collection\n      ``GraphKeys.TRAINABLE_VARIABLES`` (see `tf.Variable\n      <https://www.tensorflow.org/versions/master/api_docs/python/tf/Variable>`_).\n    name:\n      String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require ``reuse=True`` in such cases.\n    use_sparse_grads:\n      Boolean, if `True` do sparse mat mul with `embedding_lookup_sparse`, which will\n      make gradients to weight matrix also sparse in backward pass. This can lead to non-trivial\n      speed up at training time when input_size is large and optimizer handles sparse gradients\n      correctly (eg. with SGD or LazyAdamOptimizer). If weight matrix is small, it's recommended\n      to set this flag to `False`; for most use cases of FullSparse, however, weight matrix will\n      be large, so it's better to set it to `True`\n    num_partitions:\n      Number of partitions to use for the weight variable. Defaults to 1.\n    partition_axis:\n      If num_partitions is specified, the partition axis for the weight variable\n      Defaults to 0 (partition by row).\n      Must be 0 (row) or 1 (column)\n    use_binary_values:\n      Assume all non zero values are 1. Defaults to False.\n      This can improve training if used in conjunction with MDL.\n      This parameter can also be a list of binary values if `inputs` passed to `call` a list.\n    use_compression:\n      Default False. Set True to enable data compression techniques for\n      optimization of network traffic for distributed training.\n    use_binary_sparse_dense_matmul:\n      If binary sparse dense matmul op is to be used. It will only be enabled if\n      `use_binary_values` is set true. It only should be used for inference, best practice is\n      to set `use_binary_sparse_dense_matmul = not is_training`.\n  "
	def __init__(A,output_size,input_size=_A,weight_initializer=_A,activation=_A,bias_initializer=_A,trainable=_B,name=_A,use_sparse_grads=_B,num_partitions=_A,partition_axis=0,use_binary_values=_C,bias_regularizer=_A,weight_regularizer=_A,use_compression=_C,use_binary_sparse_dense_matmul=_C,**F):
		E=output_size;B=partition_axis;C=bias_initializer;D=weight_initializer;super(FullSparse,A).__init__(trainable=trainable,name=name,**F)
		if input_size:raise ValueError('input_size is deprecated - it is now automatically                        inferred from your input.')
		if C is _A:C=tf.constant_initializer(1/E)
		if D is _A:D=tf.zeros_initializer()
		A.weight_initializer=D;A.bias_initializer=C;A.output_size=E;A.activation=activation;A.use_sparse_grads=use_sparse_grads;A.num_partitions=num_partitions
		if B!=0 and B!=1:raise ValueError('partition_axis must be 0 or 1')
		A.partition_axis=B;A.use_binary_values=use_binary_values;A.weight_regularizer=weight_regularizer;A.bias_regularizer=bias_regularizer;A._use_compression=use_compression;A._cast_indices_dtype=tf.int32 if A._use_compression else _A;A.use_binary_sparse_dense_matmul=use_binary_sparse_dense_matmul
	def _make_weight_var(A,shape,partitioner):A.weight=A.add_variable('weight',initializer=A.weight_initializer,regularizer=A.weight_regularizer,shape=shape,dtype=A.dtype,trainable=_B,partitioner=partitioner)
	def build(A,input_shapes):
		'\n    creates the ``bias`` and ``weight`` Variables\n    of shape ``[output_size]`` and ``[input_size, output_size]`` respectively.\n    ';C=input_shapes
		if isinstance(C,(list,tuple)):
			D=C[0];E=_B
			for L in C[1:]:E&=D.is_compatible_with(L)
			if not E:raise ValueError('Input shapes %s are not compatible.'%C)
		else:D=C
		A.bias=A.add_variable('bias',initializer=A.bias_initializer,regularizer=A.bias_regularizer,shape=[A.output_size],dtype=A.dtype,trainable=_B);F=_A;G=[D[1],A.output_size];H=tf.as_dtype(A.dtype);I=1 if A.num_partitions is _A else A.num_partitions;B=D[1];J=A.output_size
		if isinstance(B,tf.Dimension):B=B.value
		if B is _A:raise ValueError('Input tensor should have shape. You can set it using twml.util.limit_sparse_tensor_size')
		M,N=(B,J)if A.partition_axis==0 else(J,B);K=math.ceil(float(M)/I)*N*H.size
		if K>=2**31:raise ValueError('Weight tensor partitions cannot be larger than 2GB.\nRequested Dimensions(%d, %d) of type %s (%d bytes total) over %d partitions.\nPossible solutions:\n- reduce the params.output_size_bits\n- reduce the output_size of the sparse_layer\n- specify a larger num_partitions argument\n- reduce input_size_bits'%(B,A.output_size,H.name,K,I))
		if A.num_partitions:O=int(A.partition_axis);F=tf.fixed_size_partitioner(A.num_partitions,axis=O)
		elif not callable(A.weight_initializer):G=_A
		A._make_weight_var(G,F);A.built=_B
	def compute_output_shape(A,input_shape):'Computes the output shape of the layer given the input shape.\n\n    Args:\n      input_shape: A (possibly nested tuple of) `TensorShape`.  It need not\n        be fully defined (e.g. the batch size may be unknown).\n\n    Raises NotImplementedError.\n\n    ';raise NotImplementedError
	def call(A,inputs,**G):
		'The logic of the layer lives here.\n\n    Arguments:\n      inputs:\n        A SparseTensor or a list of SparseTensors.\n        If `inputs` is a list, all tensors must have same `dense_shape`.\n\n    Returns:\n      - If `inputs` is `SparseTensor`, then returns `bias + inputs * dense_b`.\n      - If `inputs` is a `list[SparseTensor`, then returns\n        `bias + add_n([sp_a * dense_b for sp_a in inputs])`.\n\n    ';C=inputs
		if isinstance(C,(list,tuple)):
			if isinstance(A.use_binary_values,(list,tuple)):D=A.use_binary_values
			else:D=[A.use_binary_values]*len(C)
			E=len(C)
			if E!=len(D):raise ValueError('#inputs is %d while #use_binary_values is %d'%(E,len(D)))
			B=[]
			for F in range(E):B.append(sparse_dense_matmul(C[F],A.weight,A.use_sparse_grads,D[F],name='sparse_mm_'+str(F),partition_axis=A.partition_axis,num_partitions=A.num_partitions,compress_ids=A._use_compression,cast_indices_dtype=A._cast_indices_dtype,use_binary_sparse_dense_matmul=A.use_binary_sparse_dense_matmul))
			B=tf.accumulate_n(B)
		else:
			if isinstance(A.use_binary_values,(list,tuple)):raise ValueError('use_binary_values can not be %s when inputs is %s'%(type(A.use_binary_values),type(C)))
			B=sparse_dense_matmul(C,A.weight,A.use_sparse_grads,A.use_binary_values,name='sparse_mm',partition_axis=A.partition_axis,num_partitions=A.num_partitions,compress_ids=A._use_compression,cast_indices_dtype=A._cast_indices_dtype,use_binary_sparse_dense_matmul=A.use_binary_sparse_dense_matmul)
		if A.bias is not _A:B=tf.nn.bias_add(B,A.bias)
		if A.activation is not _A:return A.activation(B)
		return B
def full_sparse(inputs,output_size,input_size=_A,activation=_A,bias_regularizer=_A,weight_regularizer=_A,bias_initializer=_A,weight_initializer=_A,trainable=_B,name=_A,reuse=_A,use_sparse_grads=_B,num_partitions=_A,partition_axis=0,use_binary_values=_C,use_compression=_C):
	"Functional interface for the sparsely-connected layer.\n\n  Arguments:\n    inputs:\n      A sparse tensor (can be twml.SparseTensor or tf.SparseTensor)\n    output_size:\n      Long or Integer, dimensionality of the output space.\n    weight_initializer:\n      Initializer function for the weight matrix.\n    activation:\n      Activation function (callable). Set it to None to maintain a linear activation.\n    bias_initializer:\n      Initializer function for the bias.\n    weight_regularizer:\n      Regularizer function for the weight matrix.\n      Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.\n    bias_regularizer:\n      Regularizer function for the bias.\n      Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.\n    trainable:\n      Boolean, if `True` also add variables to the graph collection\n      ``GraphKeys.TRAINABLE_VARIABLES`` (see `tf.Variable\n      <https://www.tensorflow.org/versions/master/api_docs/python/tf/Variable>`_).\n    name:\n      String, the name of the layer. Layers with the same name will\n      share weights, but to avoid mistakes we require ``reuse=True`` in such cases.\n    use_sparse_grads:\n      Boolean, if `True` do sparse mat mul with `embedding_lookup_sparse`, which will\n      make gradients to weight matrix also sparse in backward pass. This can lead to non-trivial\n      speed up at training time when input_size is large and optimizer handles sparse gradients\n      correctly (eg. with SGD or LazyAdamOptimizer). If weight matrix is small, it's recommended\n      to set this flag to `False`; for most use cases of FullSparse, however, weight matrix will\n      be large, so it's better to set it to `True`\n    num_partitions:\n      Number of partitions to use for the weight variable. Defaults to 1.\n    partition_axis:\n      If num_partitions is specified, the partition axis for the weight variable\n      Defaults to 0 (partition by row).\n      Must be 0 (row) or 1 (column)\n    use_binary_values:\n      Assume all non zero values are 1. Defaults to False.\n      This can improve training if used in conjunction with MDL.\n    use_compression:\n      Default False. Set True to enable data compression techniques for\n      optimization of network traffic for distributed training.\n  Returns:\n    Outputs a ``tf.Tensor`` of size ``[batch_size x output_size]``.\n  ";A=inputs
	if input_size:raise ValueError('input_size is deprecated - it is now                       automatically inferred from your input.')
	B=_A
	if isinstance(A,twml.SparseTensor):A=A.to_tf();B=A.dtype.base_dtype
	if isinstance(A,(list,tuple)):A=[A.to_tf()if isinstance(A,twml.SparseTensor)else A for A in A];B=A[0].dtype.base_dtype
	C=FullSparse(output_size=output_size,activation=activation,trainable=trainable,name=name,weight_initializer=weight_initializer,bias_initializer=bias_initializer,weight_regularizer=weight_regularizer,bias_regularizer=bias_regularizer,dtype=B,_scope=name,_reuse=reuse,use_sparse_grads=use_sparse_grads,num_partitions=num_partitions,partition_axis=partition_axis,use_compression=use_compression,use_binary_values=use_binary_values);return C(A)