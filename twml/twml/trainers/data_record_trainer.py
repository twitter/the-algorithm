'\nThis module contains the ``DataRecordTrainer``.\nUnlike the parent ``Trainer`` class, the ``DataRecordTrainer``\nis used specifically for processing data records.\nIt abstracts away a lot of the intricacies of working with DataRecords.\n`DataRecord <http://go/datarecord>`_ is the main piping format for data samples.\nThe `DataRecordTrainer` assumes training data and production responses and requests\nto be organized as the `Thrift prediction service API\n\nA ``DataRecord`` is a Thrift struct that defines how to encode the data:\n\n::\n\n  struct DataRecord {\n    1: optional set<i64> binaryFeatures;                     // stores BINARY features\n    2: optional map<i64, double> continuousFeatures;         // stores CONTINUOUS features\n    3: optional map<i64, i64> discreteFeatures;              // stores DISCRETE features\n    4: optional map<i64, string> stringFeatures;             // stores STRING features\n    5: optional map<i64, set<string>> sparseBinaryFeatures;  // stores sparse BINARY features\n    6: optional map<i64, map<string, double>> sparseContinuousFeatures; // sparse CONTINUOUS feature\n    7: optional map<i64, binary> blobFeatures; // stores features as BLOBs (binary large objects)\n    8: optional map<i64, tensor.GeneralTensor> tensors; // stores TENSOR features\n    9: optional map<i64, tensor.SparseTensor> sparseTensors; // stores SPARSE_TENSOR features\n  }\n\n\nA significant portion of Twitter data is hydrated\nand then temporarily stored on HDFS as DataRecords.\nThe files are compressed (.gz or .lzo) partitions of data records.\nThese form supervised datasets. Each sample captures the relationship\nbetween input and output (cause and effect).\nTo create your own dataset, please see https://github.com/twitter/elephant-bird.\n\nThe default ``DataRecordTrainer.[train,evaluate,learn]()`` reads these datarecords.\nThe data is a read from multiple ``part-*.[compression]`` files.\nThe default behavior of ``DataRecordTrainer`` is to read sparse features from ``DataRecords``.\nThis is a legacy default piping format at Twitter.\nThe ``DataRecordTrainer`` is flexible enough for research and yet simple enough\nfor a new beginner ML practioner.\n\nBy means of the feature string to key hashing function,\nthe ``[train,eval]_feature_config`` constructor arguments\ncontrol which features can be used as sample labels, sample weights,\nor sample features.\nSamples ids, and feature keys, feature values and feature weights\ncan be skipped, included, excluded or used as labels, weights, or features.\nThis allows you to easily define and control sparse distributions of\nnamed features.\n\nYet sparse data is difficult to work with. We are currently working to\noptimize the sparse operations due to inefficiencies in the gradient descent\nand parameter update processes. There are efforts underway\nto minimize the footprint of sparse data as it is inefficient to process.\nCPUs and GPUs much prefer dense tensor data.\n'
_T='Require `initializable` if `log_tf_data_summaries`.'
_S='Expecting parse_fn to be a function.'
_R='allow_train_eval_overlap'
_Q='eval_overwrite_files_list'
_P='eval_end_datetime'
_O='eval_start_datetime'
_N='eval_data_dir'
_M='eval_files_list'
_L='train_overwrite_files_list'
_K='train_end_datetime'
_J='train_start_datetime'
_I='train_data_dir'
_H='train_files_list'
_G='%Y/%m/%d'
_F='`feature_config` was not passed to `DataRecordTrainer`'
_E='datetime_format'
_D='hour_resolution'
_C=True
_B=False
_A=None
import datetime,tensorflow.compat.v1 as tf
from twitter.deepbird.io.dal import dal_to_hdfs_path,is_dal_path
import twml
from twml.trainers import Trainer
from twml.contrib.feature_importances.feature_importances import compute_feature_importances,TREE,write_feature_importances_to_hdfs,write_feature_importances_to_ml_dash
from absl import logging
class DataRecordTrainer(Trainer):
	"\n  The ``DataRecordTrainer`` implementation is intended to satisfy the most common use cases\n  at Twitter where only the build_graph methods needs to be overridden.\n  For this reason, ``Trainer.[train,eval]_input_fn`` methods\n  assume a DataRecord dataset partitioned into part files stored in compressed (e.g. gzip) format.\n\n  For use-cases that differ from this common Twitter use-case,\n  further Trainer methods can be overridden.\n  If that still doesn't provide enough flexibility, the user can always\n  use the tf.estimator.Esimator or tf.session.run directly.\n  "
	def __init__(A,name,params,build_graph_fn,feature_config=_A,**F):
		'\n    The DataRecordTrainer constructor builds a\n    ``tf.estimator.Estimator`` and stores it in self.estimator.\n    For this reason, DataRecordTrainer accepts the same Estimator constructor arguments.\n    It also accepts additional arguments to facilitate metric evaluation and multi-phase training\n    (init_from_dir, init_map).\n\n    Args:\n      parent arguments:\n        See the `Trainer constructor <#twml.trainers.Trainer.__init__>`_ documentation\n        for a full list of arguments accepted by the parent class.\n      name, params, build_graph_fn (and other parent class args):\n        see documentation for twml.Trainer doc.\n      feature_config:\n        An object of type FeatureConfig describing what features to decode.\n        Defaults to None. But it is needed in the following cases:\n          - `get_train_input_fn()` / `get_eval_input_fn()` is called without a `parse_fn`\n          - `learn()`, `train()`, `eval()`, `calibrate()` are called without providing `*input_fn`.\n\n      **kwargs:\n        further kwargs can be specified and passed to the Estimator constructor.\n    ';super(DataRecordTrainer,A).__init__(name=name,params=params,build_graph_fn=build_graph_fn,**F);A._feature_config=feature_config;C=A.params.get(_D,1);D=A.params.get('data_threads',4);E=A.params.get(_E,_G);A._train_files=A.build_files_list(files_list_path=A.params.get(_H,_A),data_dir=A.params.get(_I,_A),start_datetime=A.params.get(_J,_A),end_datetime=A.params.get(_K,_A),datetime_format=E,data_threads=D,hour_resolution=C,maybe_save=A.is_chief(),overwrite=A.params.get(_L,_B));G=A.params.get('eval_name',_A)
		if G=='train':A._eval_files=A._train_files
		else:
			A._eval_files=A.build_files_list(files_list_path=A.params.get(_M,_A),data_dir=A.params.get(_N,_A),start_datetime=A.params.get(_O,_A),end_datetime=A.params.get(_P,_A),datetime_format=E,data_threads=D,hour_resolution=C,maybe_save=A.is_chief(),overwrite=A.params.get(_Q,_B))
			if not A.params.get(_R):
				if A._train_files and A._eval_files:B=set(A._train_files)&set(A._eval_files)
				else:B=set()
				if B:raise ValueError('There is an overlap between train and eval files:\n %s'%B)
	@staticmethod
	def build_hdfs_files_list(files_list_path,data_dir,start_datetime,end_datetime,datetime_format,data_threads,hour_resolution,maybe_save,overwrite):
		L='files';M='data_dir';N='json';O=overwrite;I='end_datetime';J='start_datetime';K=hour_resolution;G=data_dir;F=datetime_format;B=end_datetime;C=start_datetime;A=files_list_path
		if A:A=twml.util.preprocess_path(A)
		if isinstance(C,datetime.datetime):C=C.strftime(F)
		if isinstance(B,datetime.datetime):B=B.strftime(F)
		P={'base_path':G,J:C,I:B,'datetime_prefix_format':F,'extension':'lzo','parallelism':data_threads,_D:K,'sort':_C}
		if not A or not tf.io.gfile.exists(A):H=twml.util.list_files_by_datetime(**P)
		else:
			E=twml.util.read_file(A,decode=N)
			if all((A is _A for A in[G,C,B]))or E[M]==G and E[J]==C and E[I]==B and E[_E]==F and E[_D]==K:H=E[L]
			elif O:H=twml.util.list_files_by_datetime(**P)
			else:raise ValueError('Information in files_list is inconsistent with provided args.\nDid you intend to overwrite files_list using --train.overwrite_files_list or --eval.overwrite_files_list?\nIf you instead want to use the paths in files_list, ensure that data_dir, start_datetime, and end_datetime are None.')
		if maybe_save and A and(O or not tf.io.gfile.exists(A)):D={};D[L]=H;D[M]=G;D[J]=C;D[I]=B;D[_E]=F;D[_D]=K;twml.util.write_file(A,D,encode=N)
		return H
	@staticmethod
	def build_files_list(files_list_path,data_dir,start_datetime,end_datetime,datetime_format,data_threads,hour_resolution,maybe_save,overwrite):
		'\n    When specifying DAL datasets, only data_dir, start_dateime, and end_datetime\n    should be given with the format:\n\n    dal://{cluster}/{role}/{dataset_name}/{env}\n\n    ';B=overwrite;C=maybe_save;D=hour_resolution;E=data_threads;F=datetime_format;G=end_datetime;H=start_datetime;A=data_dir
		if not A or not is_dal_path(A):logging.warn(f"Please consider specifying a dal:// dataset rather than passing a physical hdfs path.");return DataRecordTrainer.build_hdfs_files_list(files_list_path,A,H,G,F,E,D,C,B)
		del F;del E;del D;del C;del B;return dal_to_hdfs_path(path=A,start_datetime=H,end_datetime=G)
	@property
	def train_files(self):return self._train_files
	@property
	def eval_files(self):return self._eval_files
	@staticmethod
	def add_parser_arguments():'\n    Add common commandline args to parse for the Trainer class.\n    Typically, the user calls this function and then parses cmd-line arguments\n    into an argparse.Namespace object which is then passed to the Trainer constructor\n    via the params argument.\n\n    See the `Trainer code <_modules/twml/trainers/trainer.html#Trainer.add_parser_arguments>`_\n    and `DataRecordTrainer code\n    <_modules/twml/trainers/trainer.html#DataRecordTrainer.add_parser_arguments>`_\n    for a list and description of all cmd-line arguments.\n\n    Args:\n      learning_rate_decay:\n        Defaults to False. When True, parses learning rate decay arguments.\n\n    Returns:\n      argparse.ArgumentParser instance with some useful args already added.\n    ';C='A float value in (0.0, 1.0] that indicates the factor by which to downsample part       files. For example, a value of 0.2 means only 20 percent of part files become part of the       dataset.';D='A float value in (0.0, 1.0] that indicates to drop records according to the Bernoulli       distribution with p = 1 - keep_rate.';B='store_true';A=super(DataRecordTrainer,DataRecordTrainer).add_parser_arguments();A.add_argument('--train.files_list','--train_files_list',type=str,default=_A,dest=_H,help='Path for a json file storing information on training data.\nSpecifically, the file at files_list should contain the dataset parameters for constructing the list of data files, and the list of data file paths.\nIf the json file does not exist, other args are used to construct the training files list, and that list will be saved to the indicated json file.\nIf the json file does exist, and current args are consistent with saved args, or are all None, then the saved files list will be used.\nIf current args are not consistent with the saved args, then error out if train_overwrite_files_list==False, else overwrite files_list with a newly constructed list.');A.add_argument('--train.overwrite_files_list','--train_overwrite_files_list',action=B,default=_B,dest=_L,help='When the --train.files_list param is used, indicates whether to overwrite the existing --train.files_list when there are differences between the current and saved dataset args. Default (False) is to error out if files_list exists and differs from current params.');A.add_argument('--train.data_dir','--train_data_dir',type=str,default=_A,dest=_I,help='Path to the training data directory.Supports local, dal://{cluster}-{region}/{role}/{dataset_name}/{environment}, and HDFS (hdfs://default/<path> ) paths.');A.add_argument('--train.start_date','--train_start_datetime',type=str,default=_A,dest=_J,help='Starting date for training inside the train data dir.The start datetime is inclusive.e.g. 2019/01/15');A.add_argument('--train.end_date','--train_end_datetime',type=str,default=_A,dest=_K,help='Ending date for training inside the train data dir.The end datetime is inclusive.e.g. 2019/01/15');A.add_argument('--eval.files_list','--eval_files_list',type=str,default=_A,dest=_M,help='Path for a json file storing information on evaluation data.\nSpecifically, the file at files_list should contain the dataset parameters for constructing the list of data files, and the list of data file paths.\nIf the json file does not exist, other args are used to construct the evaluation files list, and that list will be saved to the indicated json file.\nIf the json file does exist, and current args are consistent with saved args, or are all None, then the saved files list will be used.\nIf current args are not consistent with the saved args, then error out if eval_overwrite_files_list==False, else overwrite files_list with a newly constructed list.');A.add_argument('--eval.overwrite_files_list','--eval_overwrite_files_list',action=B,default=_B,dest=_Q,help='When the --eval.files_list param is used, indicates whether to overwrite the existing --eval.files_list when there are differences between the current and saved dataset args. Default (False) is to error out if files_list exists and differs from current params.');A.add_argument('--eval.data_dir','--eval_data_dir',type=str,default=_A,dest=_N,help='Path to the cross-validation data directory.Supports local, dal://{cluster}-{region}/{role}/{dataset_name}/{environment}, and HDFS (hdfs://default/<path> ) paths.');A.add_argument('--eval.start_date','--eval_start_datetime',type=str,default=_A,dest=_O,help='Starting date for evaluating inside the eval data dir.The start datetime is inclusive.e.g. 2019/01/15');A.add_argument('--eval.end_date','--eval_end_datetime',type=str,default=_A,dest=_P,help='Ending date for evaluating inside the eval data dir.The end datetime is inclusive.e.g. 2019/01/15');A.add_argument('--datetime_format',type=str,default=_G,help='Date format for training and evaluation datasets.Has to be a format that is understood by python datetime.e.g. %%Y/%%m/%%d for 2019/01/15.Used only if {train/eval}.{start/end}_date are provided.');A.add_argument('--hour_resolution',type=int,default=_A,help='Specify the hourly resolution of the stored data.');A.add_argument('--data_spec',type=str,required=_C,help='Path to data specification JSON file. This file is used to decode DataRecords');A.add_argument('--train.keep_rate','--train_keep_rate',type=float,default=_A,dest='train_keep_rate',help=D);A.add_argument('--eval.keep_rate','--eval_keep_rate',type=float,default=_A,dest='eval_keep_rate',help=D);A.add_argument('--train.parts_downsampling_rate','--train_parts_downsampling_rate',dest='train_parts_downsampling_rate',type=float,default=_A,help=C);A.add_argument('--eval.parts_downsampling_rate','--eval_parts_downsampling_rate',dest='eval_parts_downsampling_rate',type=float,default=_A,help=C);A.add_argument('--allow_train_eval_overlap',dest=_R,action=B,help='Allow overlap between train and eval datasets.');A.add_argument('--eval_name',type=str,default=_A,help='String denoting what we want to name the eval. If this is `train`, then we eval on       the training dataset.');return A
	def contrib_run_feature_importances(A,feature_importances_parse_fn=_A,write_to_hdfs=_C,extra_groups=_A,datarecord_filter_fn=_A,datarecord_filter_run_name=_A):
		'Compute feature importances on a trained model (this is a contrib feature)\n    Args:\n      feature_importances_parse_fn (fn): The same parse_fn that we use for training/evaluation.\n        Defaults to feature_config.get_parse_fn()\n      write_to_hdfs (bool): Setting this to True writes the feature importance metrics to HDFS\n    extra_groups (dict<str, list<str>>): A dictionary mapping the name of extra feature groups to the list of\n      the names of the features in the group\n    datarecord_filter_fn (function): a function takes a single data sample in com.twitter.ml.api.ttypes.DataRecord format\n        and return a boolean value, to indicate if this data record should be kept in feature importance module or not.\n    ';D='is_metric_larger_the_better';logging.info('Computing feature importance');E=A._params.feature_importance_algorithm;B={}
		if E==TREE:
			B['split_feature_group_on_period']=A._params.split_feature_group_on_period;B['stopping_metric']=A._params.feature_importance_metric;B['sensitivity']=A._params.feature_importance_sensitivity;B['dont_build_tree']=A._params.dont_build_tree;B['extra_groups']=extra_groups
			if A._params.feature_importance_is_metric_larger_the_better:B[D]=_C
			elif A._params.feature_importance_is_metric_smaller_the_better:B[D]=_B
			else:B[D]=_A
			logging.info('Using the tree algorithm with kwargs {}'.format(B))
		C=compute_feature_importances(trainer=A,data_dir=A._params.get('feature_importance_data_dir'),feature_config=A._feature_config,algorithm=E,record_count=A._params.feature_importance_example_count,parse_fn=feature_importances_parse_fn,datarecord_filter_fn=datarecord_filter_fn,**B)
		if not C:logging.info('Feature importances returned None')
		else:
			if write_to_hdfs:logging.info('Writing feature importance to HDFS');write_feature_importances_to_hdfs(trainer=A,feature_importances=C,output_path=datarecord_filter_run_name,metric=A._params.get('feature_importance_metric'))
			else:logging.info('Not writing feature importance to HDFS')
			logging.info('Writing feature importance to ML Metastore');write_feature_importances_to_ml_dash(trainer=A,feature_importances=C)
		return C
	def export_model(A,serving_input_receiver_fn=_A,export_output_fn=_A,export_dir=_A,checkpoint_path=_A,feature_spec=_A):
		'\n    Export the model for prediction. Typically, the exported model\n    will later be run in production servers. This method is called\n    by the user to export the PREDICT graph to disk.\n\n    Internally, this method calls `tf.estimator.Estimator.export_savedmodel\n    <https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel>`_.\n\n    Args:\n      serving_input_receiver_fn (Function):\n        function preparing the model for inference requests.\n        If not set; defaults to the the serving input receiver fn set by the FeatureConfig.\n      export_output_fn (Function):\n        Function to export the graph_output (output of build_graph) for\n        prediction. Takes a graph_output dict as sole argument and returns\n        the export_output_fns dict.\n        Defaults to ``twml.export_output_fns.batch_prediction_continuous_output_fn``.\n      export_dir:\n        directory to export a SavedModel for prediction servers.\n        Defaults to ``[save_dir]/exported_models``.\n      checkpoint_path:\n        the checkpoint path to export. If None (the default), the most recent checkpoint\n        found within the model directory ``save_dir`` is chosen.\n\n    Returns:\n      The export directory where the PREDICT graph is saved.\n    ';C=feature_spec;D=export_output_fn;B=serving_input_receiver_fn
		if B is _A:
			if A._feature_config is _A:raise ValueError(_F)
			B=A._feature_config.get_serving_input_receiver_fn()
		if C is _A:
			if A._feature_config is _A:raise ValueError('feature_spec can not be inferred.Please pass feature_spec=feature_config.get_feature_spec() to the trainer.export_model method')
			else:C=A._feature_config.get_feature_spec()
		if isinstance(B,twml.feature_config.FeatureConfig):raise ValueError('Cannot pass FeatureConfig as a parameter to serving_input_receiver_fn')
		elif not callable(B):raise ValueError('Expecting Function for serving_input_receiver_fn')
		if D is _A:D=twml.export_output_fns.batch_prediction_continuous_output_fn
		return super(DataRecordTrainer,A).export_model(export_dir=export_dir,serving_input_receiver_fn=B,checkpoint_path=checkpoint_path,export_output_fn=D,feature_spec=C)
	def get_train_input_fn(A,parse_fn=_A,repeat=_A,shuffle=_C,interleave=_C,shuffle_files=_A,initializable=_B,log_tf_data_summaries=_B,**J):
		"\n    This method is used to create input function used by estimator.train().\n\n    Args:\n      parse_fn:\n        Function to parse a data record into a set of features.\n        Defaults to the parser returned by the FeatureConfig selected\n      repeat (optional):\n        Specifies if the dataset is to be repeated. Defaults to `params.train_steps > 0`.\n        This ensures the training is run for atleast `params.train_steps`.\n        Toggling this to `False` results in training finishing when one of the following happens:\n          - The entire dataset has been trained upon once.\n          - `params.train_steps` has been reached.\n      shuffle (optional):\n        Specifies if the files and records in the files need to be shuffled.\n        When `True`,  files are shuffled, and records of each files are shuffled.\n        When `False`, files are read in alpha-numerical order. Also when `False`\n        the dataset is sharded among workers for Hogwild and distributed training\n        if no sharding configuration is provided in `params.train_dataset_shards`.\n        Defaults to `True`.\n      interleave (optional):\n        Specifies if records from multiple files need to be interleaved in parallel.\n        Defaults to `True`.\n      shuffle_files (optional):\n        Shuffle the list of files. Defaults to 'Shuffle' if not provided.\n      initializable (optional):\n        A boolean indicator. When the parsing function depends on some resource, e.g. a HashTable or\n        a Tensor, i.e. it's an initializable iterator, set it to True. Otherwise, default value\n        (false) is used for most plain iterators.\n      log_tf_data_summaries (optional):\n        A boolean indicator denoting whether to add a `tf.data.experimental.StatsAggregator` to the\n        tf.data pipeline. This adds summaries of pipeline utilization and buffer sizes to the output\n        events files. This requires that `initializable` is `True` above.\n\n    Returns:\n      An input_fn that can be consumed by `estimator.train()`.\n    ";E=log_tf_data_summaries;F=initializable;G=shuffle_files;C=shuffle;D=repeat;B=parse_fn
		if B is _A:
			if A._feature_config is _A:raise ValueError(_F)
			B=A._feature_config.get_parse_fn()
		if not callable(B):raise ValueError(_S)
		if E and not F:raise ValueError(_T)
		if D is _A:D=A.params.train_steps>0 or A.params.get('distributed',_B)
		if not C and A.num_workers>1 and A.params.train_dataset_shards is _A:H=A.num_workers;I=A.worker_index
		else:H=A.params.train_dataset_shards;I=A.params.train_dataset_shard_index
		return lambda:twml.input_fns.default_input_fn(files=A._train_files,batch_size=A.params.train_batch_size,parse_fn=B,num_threads=A.params.num_threads,repeat=D,keep_rate=A.params.train_keep_rate,parts_downsampling_rate=A.params.train_parts_downsampling_rate,shards=H,shard_index=I,shuffle=C,shuffle_files=C if G is _A else G,interleave=interleave,initializable=F,log_tf_data_summaries=E,**J)
	def get_eval_input_fn(A,parse_fn=_A,repeat=_A,shuffle=_C,interleave=_C,shuffle_files=_A,initializable=_B,log_tf_data_summaries=_B,**H):
		"\n    This method is used to create input function used by estimator.eval().\n\n    Args:\n      parse_fn:\n        Function to parse a data record into a set of features.\n        Defaults to twml.parsers.get_sparse_parse_fn(feature_config).\n      repeat (optional):\n        Specifies if the dataset is to be repeated. Defaults to `params.eval_steps > 0`.\n        This ensures the evaluation is run for atleast `params.eval_steps`.\n        Toggling this to `False` results in evaluation finishing when one of the following happens:\n          - The entire dataset has been evaled upon once.\n          - `params.eval_steps` has been reached.\n      shuffle (optional):\n        Specifies if the files and records in the files need to be shuffled.\n        When `False`, files are read in alpha-numerical order.\n        When `True`,  files are shuffled, and records of each files are shuffled.\n        Defaults to `True`.\n      interleave (optional):\n        Specifies if records from multiple files need to be interleaved in parallel.\n        Defaults to `True`.\n      shuffle_files (optional):\n        Shuffles the list of files. Defaults to 'Shuffle' if not provided.\n      initializable (optional):\n        A boolean indicator. When the parsing function depends on some resource, e.g. a HashTable or\n        a Tensor, i.e. it's an initializable iterator, set it to True. Otherwise, default value\n        (false) is used for most plain iterators.\n      log_tf_data_summaries (optional):\n        A boolean indicator denoting whether to add a `tf.data.experimental.StatsAggregator` to the\n        tf.data pipeline. This adds summaries of pipeline utilization and buffer sizes to the output\n        events files. This requires that `initializable` is `True` above.\n\n    Returns:\n      An input_fn that can be consumed by `estimator.eval()`.\n    ";D=log_tf_data_summaries;E=initializable;F=shuffle_files;G=shuffle;C=repeat;B=parse_fn
		if B is _A:
			if A._feature_config is _A:raise ValueError(_F)
			B=A._feature_config.get_parse_fn()
		if not A._eval_files:raise ValueError('`eval_files` was not present in `params` passed to `DataRecordTrainer`')
		if not callable(B):raise ValueError(_S)
		if D and not E:raise ValueError(_T)
		if C is _A:C=A.params.eval_steps>0
		return lambda:twml.input_fns.default_input_fn(files=A._eval_files,batch_size=A.params.eval_batch_size,parse_fn=B,num_threads=A.params.num_threads,repeat=C,keep_rate=A.params.eval_keep_rate,parts_downsampling_rate=A.params.eval_parts_downsampling_rate,shuffle=G,shuffle_files=G if F is _A else F,interleave=interleave,initializable=E,log_tf_data_summaries=D,**H)
	def _assert_train_files(A):
		if not A._train_files:raise ValueError('train.data_dir was not set in params passed to DataRecordTrainer.')
	def _assert_eval_files(A):
		if not A._eval_files:raise ValueError('eval.data_dir was not set in params passed to DataRecordTrainer.')
	def train(B,input_fn=_A,steps=_A,hooks=_A):
		'\n    Makes input functions optional. input_fn defaults to self.get_train_input_fn().\n    See Trainer for more detailed documentation documentation.\n    ';A=input_fn
		if A is _A:B._assert_train_files()
		A=A if A else B.get_train_input_fn();super(DataRecordTrainer,B).train(input_fn=A,steps=steps,hooks=hooks)
	def evaluate(B,input_fn=_A,steps=_A,hooks=_A,name=_A):
		'\n    Makes input functions optional. input_fn defaults to self.get_eval_input_fn().\n    See Trainer for more detailed documentation.\n    ';A=input_fn
		if A is _A:B._assert_eval_files()
		A=A if A else B.get_eval_input_fn(repeat=_B);return super(DataRecordTrainer,B).evaluate(input_fn=A,steps=steps,hooks=hooks,name=name)
	def learn(A,train_input_fn=_A,eval_input_fn=_A,**D):
		'\n    Overrides ``Trainer.learn`` to make ``input_fn`` functions optional.\n    Respectively, ``train_input_fn`` and ``eval_input_fn`` default to\n    ``self.train_input_fn`` and ``self.eval_input_fn``.\n    See ``Trainer.learn`` for more detailed documentation.\n    ';B=eval_input_fn;C=train_input_fn
		if C is _A:A._assert_train_files()
		if B is _A:A._assert_eval_files()
		C=C if C else A.get_train_input_fn();B=B if B else A.get_eval_input_fn();super(DataRecordTrainer,A).learn(train_input_fn=C,eval_input_fn=B,**D)
	def train_and_evaluate(A,train_input_fn=_A,eval_input_fn=_A,**D):
		'\n    Overrides ``Trainer.train_and_evaluate`` to make ``input_fn`` functions optional.\n    Respectively, ``train_input_fn`` and ``eval_input_fn`` default to\n    ``self.train_input_fn`` and ``self.eval_input_fn``.\n    See ``Trainer.train_and_evaluate`` for detailed documentation.\n    ';B=eval_input_fn;C=train_input_fn
		if C is _A:A._assert_train_files()
		if B is _A:A._assert_eval_files()
		C=C if C else A.get_train_input_fn();B=B if B else A.get_eval_input_fn();super(DataRecordTrainer,A).train_and_evaluate(train_input_fn=C,eval_input_fn=B,**D)
	def _model_fn(A,features,labels,mode,params,config=_A):
		'\n    Overrides the _model_fn to correct for the features shape of the sparse features\n    extracted with the contrib.FeatureConfig\n    ';B=features
		if isinstance(A._feature_config,twml.contrib.feature_config.FeatureConfig):twml.util.fix_shape_sparse(B,A._feature_config)
		return super(DataRecordTrainer,A)._model_fn(features=B,labels=labels,mode=mode,params=params,config=config)
	def calibrate(B,calibrator,input_fn=_A,steps=_A,save_calibrator=_C,hooks=_A):
		'\n    Makes input functions optional. input_fn defaults to self.train_input_fn.\n    See Trainer for more detailed documentation.\n    ';A=input_fn
		if A is _A:B._assert_train_files()
		A=A if A else B.get_train_input_fn();super(DataRecordTrainer,B).calibrate(calibrator=calibrator,input_fn=A,steps=steps,save_calibrator=save_calibrator,hooks=hooks)
	def save_checkpoints_and_export_model(A,serving_input_receiver_fn,export_output_fn=_A,export_dir=_A,checkpoint_path=_A,input_fn=_A):'\n    Exports saved module after saving checkpoint to save_dir.\n    Please note that to use this method, you need to assign a loss to the output\n    of the build_graph (for the train mode).\n    See export_model for more detailed information.\n    ';A.train(input_fn=input_fn,steps=1);A.export_model(serving_input_receiver_fn,export_output_fn,export_dir,checkpoint_path)
	def save_checkpoints_and_evaluate(A,input_fn=_A,steps=_A,hooks=_A,name=_A):'\n    Evaluates model after saving checkpoint to save_dir.\n    Please note that to use this method, you need to assign a loss to the output\n    of the build_graph (for the train mode).\n    See evaluate for more detailed information.\n    ';B=input_fn;A.train(input_fn=B,steps=1);A.evaluate(B,steps,hooks,name)