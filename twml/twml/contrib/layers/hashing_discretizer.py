'\nImplementing HashingDiscretizer Layer\n'
import libtwml,tensorflow.compat.v1 as tf,twml
from twml.constants import HashingDiscretizerOptions
from twml.layers.layer import Layer
class HashingDiscretizer(Layer):
	'A layer that discretizes continuous features, with hashed feature assignments\n\n  HashingDiscretizer converts sparse continuous features into sparse\n  binary features. Each binary output feature indicates the presence of a\n  value in a HashingDiscretizer bin.\n\n  Each calibrated HashingDiscretizer input feature is converted to n_bin+1 bins.\n\n  - n_bin bin boundaries for each feature (i.e. len(bin_vals[id])==n_bin) defines n_bin+1 bins\n  - bin assignment = sum(bin_vals<val)\n\n  The difference between this layer and PercentileDiscretizer is that the\n  HashingDiscretizer always assigns the same output id in the\n  SparseTensor to the same input (feature id, bin) pair. This is useful if you\n  want to user transfer learning on pre-trained sparse to dense embedding\n  layers, but re-calibrate your discretizer on newer data.\n\n  If there are no calibrated features, then the discretizer will only apply\n  twml.util.limit_bits to the the feature keys (aka "feature_ids"). Essentially,\n  the discretizer will be a "no-operation", other than obeying `out_bits`\n\n  Typically, a HashingDiscretizer layer will be generated by calling the\n  to_layer() method of the HashingDiscretizerCalibrator\n  '
	def __init__(A,feature_ids,bin_vals,n_bin,out_bits,cost_per_unit=500,options=None,**C):
		'\n    Creates a non-initialized `HashingDiscretizer` object.\n\n    Parent class args:\n      see [tf.layers.Layer](https://www.tensorflow.org/api_docs/python/tf/layers/Layer)\n      for documentation of parent class arguments.\n\n    Required args:\n      feature_ids (1D int64 numpy array):\n      - list of feature IDs that have been calibrated and have corresponding\n        bin boundary values in the bin_vals array\n      - bin values for feature feature_ids[i] live at bin_vals[i*n_bin:(i+1)*n_bin]\n      bin_vals (1D float numpy array):\n      - These are the bin boundary values for each calibrated feature\n      - len(bin_vals) = n_bin*len(feature_ids)\n      n_bin (int):\n      - number of HashingDiscretizer bins is actually n_bin + 1\n      - ***Note*** that if a value N is passed for the value of n_bin to\n        HashingDiscretizerCalibrator, then HashingDiscretizerCalibrator\n        will generate N+1 bin boundaries for each feature, and hence there\n        will actually be N+2 potential bins for each feature\n      out_bits (int):\n        Determines the maximum value for output feature IDs.\n        The dense_shape of the SparseTensor returned by lookup(x)\n        will be [x.shape[0], 1 << output_bits].\n\n    Optional args:\n      cost_per_unit (int):\n      - heuristic for intra op multithreading. approximate nanoseconds per input value.\n      options (int or None for default):\n      - Selects behavior of the op. Default is lower_bound and integer_multiplicative_hashing.\n      - Use values in twml.constants.HashingDiscretizerOptions to select options as follows\n        choose exactly one of HashingDiscretizerOptions.{SEARCH_LOWER_BOUND, SEARCH_LINEAR, SEARCH_UPPER_BOUND}\n        choose exactly one of HashingDiscretizerOptions.{HASH_32BIT, HASH_64BIT}\n        Bitwise OR these together to construct the options input.\n        For example, `options=(HashingDiscretizerOptions.SEARCH_UPPER_BOUND | HashingDiscretizerOptions.HASH_64BIT)`\n    ';B=options;super(HashingDiscretizer,A).__init__(**C);A._feature_ids=feature_ids;A._bin_vals=bin_vals;A._n_bin=n_bin;A._out_bits=out_bits;A.cost_per_unit=cost_per_unit
		if B is None:B=HashingDiscretizerOptions.SEARCH_LOWER_BOUND|HashingDiscretizerOptions.HASH_32BIT
		A._options=B
		if not A.built:A.build(input_shape=None)
	def build(A,input_shape):'\n    Creates the variables of the layer\n    ';A.built=True
	def call(A,inputs,**K):
		'\n    Implements HashingDiscretizer inference on a twml.SparseTensor.\n    Alternatively, accepts a tf.SparseTensor that can be converted\n    to twml.SparseTensor.\n\n    Performs discretization of input values.\n    i.e. bucket_val = bucket(val | feature_id)\n\n    This bucket mapping depends on the calibration (i.e. the bin boundaries).\n    However, (feature_id, bucket_val) pairs are mapped to new_feature_id in\n    a way that is independent of the calibration procedure\n\n    Args:\n      inputs: A 2D SparseTensor that is input to HashingDiscretizer for\n        discretization. It has a dense_shape of [batch_size, input_size]\n      name: A name for the operation (optional).\n    Returns:\n      A tf.SparseTensor, created from twml.SparseTensor.to_tf()\n      Its dense_shape is [shape_input.dense_shape[0], 1 << output_bits].\n    ';B=inputs
		if isinstance(B,tf.SparseTensor):B=twml.SparseTensor.from_tf(B)
		assert isinstance(B,twml.SparseTensor);G=B.ids;C=B.indices;D=B.values
		if len(A._feature_ids)>0:E,F=libtwml.ops.hashing_discretizer(input_ids=C,input_vals=D,bin_vals=A._bin_vals,feature_ids=tf.make_tensor_proto(A._feature_ids),n_bin=A._n_bin,output_bits=A._out_bits,cost_per_unit=A.cost_per_unit,options=A._options)
		else:E=twml.util.limit_bits(C,A._out_bits);F=D
		H=tf.to_int64(B.dense_shape[0]);I=tf.convert_to_tensor(1<<A._out_bits,tf.int64);J=[H,I];return twml.SparseTensor(G,E,F,J).to_tf()